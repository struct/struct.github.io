<!DOCTYPE html>
<html>
  <head>
    <script src="javascripts/scale.fix.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1ZHQ6N5ZEZ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-1ZHQ6N5ZEZ');
    </script>
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Root Cause</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href='http://struct.github.io'>Root Cause</a></h1>
        <p></p>
        <p class="view">
        <a href="https://twitter.com/chrisrohlf"><img height="32" width="32" src="icons/twitter.svg" /></a>&nbsp;
        <a href="https://github.com/struct"><img height="32" width="32" src="icons/github.svg" /></a>&nbsp;
        <a href="https://www.linkedin.com/in/chrisrohlf"><img height="32" width="32" src="icons/linkedin.svg" /></a>&nbsp;
        <a href="mailto:chris dot rohlf _at_ gmail.com"><img height="32" width="32" src="icons/gmail.svg" /></a>
        </p>
      </header>
      <section>
        <h2><div id='post0'>LLM Emergent Abilities and Weird Machines</div></h2>
        <p><script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
          October 4th, 2024<br>
          Chris Rohlf
          <br><br>
          AI scaling laws generally describe how the performance of Large Language Models (LLMs) improves somewhat predictably with increases in several key factors: 
          1) pre-training computational resources, 2) model parameter count, 3) training token count, and now demonstrated by OpenAI's o1 4) inference computational resources. An interesting 
          consequence of these scaling laws is what is known as "<a href="https://cset.georgetown.edu/article/emergent-abilities-in-large-language-models-an-explainer/">Emergent Abilities</a>". 
          These are capabilities that were not explicitly programmed into the model but arise naturally as a result of scaling and generalizing over vast amounts of 
          pretraining data. In short, an LLM's abilities are not so much designed as they are discovered. This represents a fundamental and important distinction from 
          traditional software which is designed and programmed with an explicit and intended set of functionalities. Much of this functionality can be tested for, and sometimes
          even formally verified against a specification.
          <br><br>
          When it comes to LLM abilities, particularly those relevant to cybersecurity, these are often dual-use, meaning they can be employed for both beneficial 
          and malicious purposes depending on the actors intent. As we've previously discussed, we don't always immediately know the full extent of what 
          these models are capable of. This uncertainty necessitates the use of various benchmarks and 
          <a href="https://ai.meta.com/research/publications/cyberseceval-3-advancing-the-evaluation-of-cybersecurity-risks-and-capabilities-in-large-language-models/">
            evaluations</a> with each new model release to 
          systematically assess their capabilities and the potential uplift for both attacker and defender.
          <br><br>
          However, there is always the possibility that a threat actor could discover an emergent ability that remains unknown to others. Whether that 
          ability is dual-use or not becomes largely irrelevant if good actors have no knowledge of it. At first glance, this scenario seems novel and 
          unique to LLMs. However, traditional software can also contain errors, often referred to as 0-day vulnerabilities, that allow the software to 
          be reprogrammed, or exploited, in unintended ways. The exploitation of these vulnerabilities can enable the creation of 
          <a href="https://en.wikipedia.org/wiki/Weird_machine">weird machines</a>. These weird machines are created through combining unintended computational 
          artifacts that become activated given the right set of inputs. They can be thought of as emerging in the sense that they accumulate with complexity
          in ways that are often hard to predict.
          <br><br>
          It is often the case that threat actors discover these vulnerabilities long before they are known to the authors of the code or the general 
          public, leveraging them to craft weird machines and ultimately compromise their targets. The comparison between emergent abilities in LLMs 
          and weird machines in traditional software is an interesting one, as it largely boils down to unintended properties of complex systems. 
          However, the comparison is also somewhat flawed. For instance, emergent abilities are often generalized and reusable and often improve the models 
          usefulness, whereas vulnerabilities in software are typically seen as regrettable errors to be fixed. Emergent abilities in LLMs are often 
          thought of as breakthroughs, even if they present some risks in a dual-use context.
          <br><br>
          However flawed the comparison between emergent abilities in LLMs and weird machines is it underscores the 
          unpredictable and often unknowable properties inherent in any complex system that is often too large to formally assess with any precision. Both 
          involve functionality that was not intentionally designed by its developers but can be discovered and potentially utilized by good and bad 
          actors alike. Recognizing the parallels and distinctions between these two is important when evaluating and debating the risks associated with 
          making new models available. While LLMs are ushering in a new computing paradigm that seems to grow more capable as they are scaled, there are still 
          valuable lessons to be applied from traditional threat modeling and the many flawed assumptions we've made regarding human authored software 
          and how well we are able to reason about its total set of functionality, intended or otherwise.
          </p>
      </section>
    </div>
  </body>
</html>